import urllib.request 
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk
import re
import inflect
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS
from collections import Counter

nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
def open_url(url):
    file = urllib.request.urlopen(url)
    decoded_line = ''
    for line in file:
	    decoded_line += line.decode("utf-8")
    return decoded_line
def discard(text):
    # removing starting and ending portion that is unwanted
    start = text.find('*** START OF THE PROJECT')
    end = text.find('*** END OF THE PROJECT')
    text = text[start:end]
    # removing links from text
    re.sub(r'http\S+', '', text)
    return text
def lower_case(text: str):
    return text.lower()
def lemmanization(text):
    arr = text.split()
    array = []
    lemmatizer = WordNetLemmatizer()
    for i in arr:
        array.append(lemmatizer.lemmatize(i))
    return ' '.join(array)
def remove_tags(text):
    TAG_RE = re.compile(r'<[^>]+>')
    return TAG_RE.sub('', text)
def remove_punctuations(text):
    punc = '''!()-[]{};:'"\,<>./?’@”#$%^“&*_~'''
    for ele in text:
        if ele in punc:
            text = text.replace(ele, "")
    return text
def tokenization(text):
    return nltk.word_tokenize(text)
def stemming(text):
    tokens=tokenization(text)
    arr = []
    ps = PorterStemmer()
    for i in tokens:
        arr.append(ps.stem(i))
    return ' '.join(arr)
def remove_stop_words(text):
    # return [word for word in tokens if word not in STOPWORDS]
    tokens=tokenization(text)
    arr = []
    stop_words = set(stopwords.words('english'))
    for i in tokens:
        if i not in stop_words:
            arr.append(i)
    return ' '.join(arr)
 def frequency_distribution(text):
    tokens = word_tokenize(text) 
    pd.Series(tokens).value_counts()[:20].plot(kind='bar')
def word_cloud_plot(text):
    tokens = tokenization(text)
    wordcloud = WordCloud(width=1600, height=800, max_font_size=200, background_color="black").generate(text)
    # plt the image generated by WordCloud class
    plt.figure(figsize=(12,10))
    plt.imshow(wordcloud)
    plt.axis("off")
    plt.show()
def word_length_frequency(text):
    token = tokenization(text)
    dic = list(set(token))
    length = [len(i) for i in token]
    pd.Series(length).value_counts()[:20].plot(kind='bar')
def frequency_distribution_of_tags(tags):
    tags = [i[1] for i in tags]
    count = Counter(tags)
    x = [i for i in count]
    y = [count[i] for i in count]
    plt.plot(x, y)
    plt.show()
def pos_tagging(text):
    tokens = tokenization(text)
    tag = nltk.pos_tag(tokens)
    return tag
url1 = 'https://www.gutenberg.org/cache/epub/66774/pg66774.txt'
text1 = open_url(url1)
url2 = 'https://www.gutenberg.org/cache/epub/66766/pg66766.txt'
text2 = open_url(url2)
# print(text2)
def preprocessing(text):
    text=discard(text)
    text=lower_case(text)
    text=remove_tags(text)
    text=lemmanization(text)
    text=remove_punctuations(text)
    text=stemming(text)
    return text
text1 = preprocessing(text1)
text2 = preprocessing(text2)
# frequency_distribution(text1)
# frequency_distribution(text2)
# word_cloud_plot(text1)
# word_cloud_plot(text2)
text1 = remove_stop_words(text1)
text2 = remove_stop_words(text2)
# word_cloud_plot(text1)
# word_cloud_plot(text2)
# word_length_frequency(text1)
# word_length_frequency(text2)
tag1 = pos_tagging(text1)
tag2 = pos_tagging(text2)
# frequency_distribution_of_tags(tag1)
# frequency_distribution_of_tags(tag2)
